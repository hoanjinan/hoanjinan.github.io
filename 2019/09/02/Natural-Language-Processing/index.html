<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Natural Language Processing (NLP) | hoanjinan_otoko</title><meta name="description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatizing, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta name="keywords" content="Data Science,Data Processing,Machine Learning,AI"><meta name="author" content="Zilan Huang"><meta name="copyright" content="Zilan Huang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Natural Language Processing (NLP)"><meta name="twitter:description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatizing, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta name="twitter:image" content="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Natural Language Processing (NLP)"><meta property="og:url" content="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/"><meta property="og:site_name" content="hoanjinan_otoko"><meta property="og:description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatizing, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta property="og:image" content="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="Spark for Machine Learning and AI" href="http://hoanjinan.github.io/2019/08/20/Spark-for-Machine-Learning-AI/"><link rel="manifest" href="/img/pwa/manifest.json"><meta name="theme-color" content="#fff"><meta name="msapplication-TileColor" content="#fff"><link rel="apple-touch-icon" sizes="180x180" href="/img/pwa/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/pwa/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/pwa/16.png"><link rel="mask-icon" href="/img/pwa/safari-pinned-tab.svg" color="#5bbad5"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a29c9ae6807bf668430dda20bfdf3ac5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-146505841-1', 'auto');
ga('send', 'pageview');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction-to-the-post"><span class="toc-number">1.</span> <span class="toc-text"> Introduction to The Post</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction-to-natural-language-processing-nlp"><span class="toc-number">2.</span> <span class="toc-text"> Introduction to Natural Language Processing (NLP)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction-to-nltk"><span class="toc-number">3.</span> <span class="toc-text"> Introduction to NLTK</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#how-to-install-nltk-on-your-local-machine"><span class="toc-number">4.</span> <span class="toc-text"> How to install NLTK on your local machine</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#download-nltk-data"><span class="toc-number">4.1.</span> <span class="toc-text"> Download NLTK data</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#reading-in-text-data"><span class="toc-number">5.</span> <span class="toc-text"> Reading in Text Data</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-unstructured-data"><span class="toc-number">5.1.</span> <span class="toc-text"> What is Unstructured Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#read-semi-structured-data"><span class="toc-number">5.2.</span> <span class="toc-text"> Read Semi-structured data</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#exploring-the-dataset"><span class="toc-number">6.</span> <span class="toc-text"> Exploring The Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#regular-expressions"><span class="toc-number">7.</span> <span class="toc-text"> Regular Expressions</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-regular-expression"><span class="toc-number">7.1.</span> <span class="toc-text"> Introduction to Regular Expression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-use-regular-expressions"><span class="toc-number">7.2.</span> <span class="toc-text"> How to Use Regular Expressions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regular-expression-replacements"><span class="toc-number">7.3.</span> <span class="toc-text"> Regular Expression Replacements</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#other-examples-of-regex-methods"><span class="toc-number">7.3.1.</span> <span class="toc-text"> Other examples of regex methods</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#machine-learning-pipeline"><span class="toc-number">8.</span> <span class="toc-text"> Machine Learning Pipeline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#implementation"><span class="toc-number">9.</span> <span class="toc-text"> Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#removing-punctuation"><span class="toc-number">9.1.</span> <span class="toc-text"> Removing Punctuation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tokenisation"><span class="toc-number">9.2.</span> <span class="toc-text"> Tokenisation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#removing-stop-words"><span class="toc-number">9.3.</span> <span class="toc-text"> Removing Stop Words</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://live.staticflickr.com/65535/48621182418_494606b3cf_o.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">hoanjinan_otoko</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Natural Language Processing (NLP)</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-09-02<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-09-05</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Tutorial/">Tutorial</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Tutorial/Data-Processing/">Data Processing</a></span><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">6.6k</span><span class="post-meta__separator">|</span><span>Reading time: 41 min</span><span class="post-meta__separator">|</span><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="introduction-to-the-post"><a class="markdownIt-Anchor" href="#introduction-to-the-post"></a> Introduction to The Post</h1>
<p>Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you’re trying to type?  In this post, we’ll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatizing, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We’ll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems.</p>
<h1 id="introduction-to-natural-language-processing-nlp"><a class="markdownIt-Anchor" href="#introduction-to-natural-language-processing-nlp"></a> Introduction to Natural Language Processing (NLP)</h1>
<p>Natural language processing is a field concerned with the ability of a computer to understand, analyze, manipulate, and potentially generate human language. By human language, we’re simply referring to any language used for everyday communication.  This can be English, Spanish, French, anything like that.  Now it’s worth noting that Python doesn’t naturally know what any given word means.  All it will see is a string of characters.  For instance, it has no idea what natural actually means.  It sees that it’s seven characters long, but the individual characters don’t mean anything to Python and certainly the collection of those characters together don’t mean anything, either.  So we know that, what an N is, what an A is, and we know that together, those seven characters makes up the word natural, and we know what that means.  So NLP is the field of getting the computer to understand what naturally actually signifies, and from there we can get into the manipulation or potentially even generation of that human language.</p>
<p>You probably experience natural language processing on a daily basis.  They may not really even know it.  So here are a few examples that you may see on a day to day basis.  The first would be a spam filter, so this is just where your email server is determining whether an incoming email is spam or not, based on the content of the body, the subject, and maybe the email domain.  The second is auto-complete, where Google is basically predicting what you’re interested in searching for based on what you’ve already entered and what others commonly search for with those same phrases.  So if I search for natural language processing, it knows that many other people are interested in learning NLP with Python, or learning it through a course, or looking for jobs related to natural language processing.  So it can auto-complete your search for you.  The last is auto-correct, where say iPhone is trying to help you correct a misspelling. It shows how auto-correct has actually evolved over time and continues to evolve and learn by upgrading the operating system.  So with iOS 6, if you’re trying to say, “I’ll be ill tomorrow,” It wouldn’t necessarily correct I’ll be I’ll tomorrow until iOS 7, where it actually corrects, it auto-completes tomorrow and corrects I’ll into ill.  So it’ll correctly send as I’ll be ill tomorrow.  So that just kind of shows how NLP is still evolving and how a system like iOS is still kind of learning what natural language even means.</p>
<p>Now NLP is a very broad umbrella that encompasses many topics. A few of those might be sentiment analysis, topic modeling, text classification, and sentence segmentation or part-or-speech tagging.  The core component of natural language processing is extracting all the information from a block of text that is relevant to a computer understanding the language.  This is task specific, as well.  Different information is relevant for a sentiment analysis task than is relevant for a topic modeling task. So that’s a very quick introduction into what natural language processing is.</p>
<h1 id="introduction-to-nltk"><a class="markdownIt-Anchor" href="#introduction-to-nltk"></a> Introduction to NLTK</h1>
<p>The natural language toolkit is the most utilized package for handling natural language processing tasks in Python.  Usually called NLTK for short, it is a suite of open-source tools originally created in 2001 at the University of Pennsylvania for the purpose of making building NLP processes in Python easier.  This package has been expanded through the extensive contributions of open-source users in the years since its original development.  NLTK is great because it basically provides a jumpstart to building any NLP process by giving you the basic tools that you can then chain together to accomplish your goal rather than having to build all those tools from scratch and a lot of tools are packaged into NLTK.</p>
<h1 id="how-to-install-nltk-on-your-local-machine"><a class="markdownIt-Anchor" href="#how-to-install-nltk-on-your-local-machine"></a> How to install NLTK on your local machine</h1>
<p>Both sets of instructions below assume you already have Python installed. These instructions are taken directly from <a href="http://www.nltk.org/install.html" target="_blank" rel="noopener">http://www.nltk.org/install.html</a>.</p>
<p><strong>Mac/Unix</strong></p>
<p>From the terminal:</p>
<ol>
<li>Install NLTK: run <code>pip install -U nltk</code></li>
<li>Test installation: run <code>python</code> then type <code>import nltk</code></li>
</ol>
<p><strong>Windows</strong></p>
<ol>
<li>Install NLTK: <a href="http://pypi.python.org/pypi/nltk" target="_blank" rel="noopener">http://pypi.python.org/pypi/nltk</a></li>
<li>Test installation: <code>Start&gt;Python35</code>, then type <code>import nltk</code></li>
</ol>
<h2 id="download-nltk-data"><a class="markdownIt-Anchor" href="#download-nltk-data"></a> Download NLTK data</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download()</span><br></pre></td></tr></table></figure>
<p>The following images will show the downloader of NLTK:</p>
<p><img alt="NLTK All Packages Uninstalled" data-src="/img/NLP/NLTK_Uninstalled.png" class="lozad"></p>
<p>In above image, select ‘all packages’ and click ‘download’ button to start installing all NLTK packages.</p>
<p><img alt="NLTK All Packages Installed" data-src="/img/NLP/NLTK_Installed.png" class="lozad"></p>
<p>The above image shows that all NLTK packages have been installed.</p>
<p>Small example of using NLTK package:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stopwords.words(<span class="string">"english"</span>)[<span class="number">0</span>:<span class="number">500</span>:<span class="number">25</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'i'</span>, <span class="string">'herself'</span>, <span class="string">'been'</span>, <span class="string">'with'</span>, <span class="string">'here'</span>, <span class="string">'very'</span>, <span class="string">'doesn'</span>, <span class="string">'won'</span>]</span><br></pre></td></tr></table></figure>
<h1 id="reading-in-text-data"><a class="markdownIt-Anchor" href="#reading-in-text-data"></a> Reading in Text Data</h1>
<h2 id="what-is-unstructured-data"><a class="markdownIt-Anchor" href="#what-is-unstructured-data"></a> What is Unstructured Data</h2>
<p>Unstructured data could mean that it’s binary data, it could mean no delimiters, or it could mean no indications of any rows.  A few examples might be an email, PDF file, social media post, these may just get dumped into a file with no indication of where, maybe, a subject of an email ends and the body of the email begins, or even where one email ends and the next begins.  It could also get cluttered by things like HTML tags and it can get really messy.  It’s important to note that Python is pretty smart, but ultimately, unless it’s told otherwise, it basically sees everything as a string of characters. It needs to be told what those characters mean.</p>
<h2 id="read-semi-structured-data"><a class="markdownIt-Anchor" href="#read-semi-structured-data"></a> Read Semi-structured data</h2>
<p>The following image presents a semi-structured SMS data:</p>
<p><img alt="Semi-structured SMS data sample" data-src="/img/NLP/SMS.png" class="lozad"></p>
<p>This dataset is a collection of text messages, each with a label of either spam or ham.  It’s not a clean CSV file, but it’s not terribly unstructured, either.  Each row has a distinct text message and a distinct label as either spam or ham. So, in the context of text datasets, this is actually pretty well structured, so this shouldn’t be too difficult.</p>
<p>Reading the data and print it out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Read in the raw text</span></span><br><span class="line">rawData = open(<span class="string">"SMSSpamCollection.tsv"</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the raw data</span></span><br><span class="line">rawData[<span class="number">0</span>:<span class="number">500</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\nspam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's\nham\tNah I don't think he goes to usf, he lives around here though\nham\tEven my brother is not like to speak with me. They treat me like aid"</span></span><br></pre></td></tr></table></figure>
<p>You could see that it’s just basically a block of text, and you’ll see that you have these \t and these \n separators. The \t’s are between the labels and the text message bodies, and the \n’s are typically at the end of those lines.</p>
<p>The following code is going to replace \n with \t and then split this into a list:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parsedData = rawData.replace(<span class="string">"\t"</span>, <span class="string">"\n"</span>).split(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">parsedData[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>,</span><br><span class="line"> <span class="string">"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."</span>,</span><br><span class="line"> <span class="string">'spam'</span>,</span><br><span class="line"> <span class="string">"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"</span>,</span><br><span class="line"> <span class="string">'ham'</span>]</span><br></pre></td></tr></table></figure>
<p>Split the label and the text into lists:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labelList = parsedData[<span class="number">0</span>::<span class="number">2</span>]</span><br><span class="line">textList = parsedData[<span class="number">1</span>::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>Print the results:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(labelList[<span class="number">0</span>:<span class="number">5</span>])</span><br><span class="line">print(textList[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>, <span class="string">'spam'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>]</span><br><span class="line">[<span class="string">"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."</span>, <span class="string">"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"</span>, <span class="string">"Nah I don't think he goes to usf, he lives around here though"</span>, <span class="string">'Even my brother is not like to speak with me. They treat me like aids patent.'</span>, <span class="string">'I HAVE A DATE ON SUNDAY WITH WILL!!'</span>]</span><br></pre></td></tr></table></figure>
<p>Combine both lists and put them into pandas dataframe:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fullCorpus = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"label"</span>: labelList,</span><br><span class="line">    <span class="string">"body_list"</span>: textList</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: arrays must all be same length</span><br></pre></td></tr></table></figure>
<p>Spot the error:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(len(labelList))</span><br><span class="line">print(len(textList))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5571</span></span><br><span class="line"><span class="number">5570</span></span><br></pre></td></tr></table></figure>
<p>Print the last 5 values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(labelList[<span class="number">-5</span>:])</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure>
<p>Correction:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fullCorpus = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"label"</span>: labelList[:<span class="number">-1</span>],</span><br><span class="line">    <span class="string">"body_list"</span>: textList</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>Read the file using pandas:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep = <span class="string">"\t"</span>, header = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">dataset.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<h1 id="exploring-the-dataset"><a class="markdownIt-Anchor" href="#exploring-the-dataset"></a> Exploring The Dataset</h1>
<p>Before diving into any in-depth analysis, data cleaning or model building, we want to do some very high-level exploration of our data to understand what we’re working with.  So we might ask questions like what is the shape of our data, how many ham or spam are in our data set, and are there any missing values.  So this will inform the decisions that we make as we move forward.</p>
<p>Read the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fullCorpus = pd.read_csv(<span class="string">'SMSSpamCollection.tsv'</span>, sep=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">fullCorpus.columns = [<span class="string">"label"</span>, <span class="string">"body_text"</span>]</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>What is the shape of the dataset?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Input data has &#123;&#125; rows and &#123;&#125; columns"</span>.format(len(fullCorpus), len(fullCorpus.columns)))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Input data has <span class="number">5568</span> rows <span class="keyword">and</span> <span class="number">2</span> columns</span><br></pre></td></tr></table></figure>
<p>How many spam/ham are there?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Out of &#123;&#125; rows, &#123;&#125; are spam, &#123;&#125; are ham"</span>.format(len(fullCorpus),</span><br><span class="line">                                                       len(fullCorpus[fullCorpus[<span class="string">"label"</span>] == <span class="string">"spam"</span>]),</span><br><span class="line">                                                       len(fullCorpus[fullCorpus[<span class="string">"label"</span>] == <span class="string">"ham"</span>])))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Out of <span class="number">5568</span> rows, <span class="number">746</span> are spam, <span class="number">4822</span> are ham</span><br></pre></td></tr></table></figure>
<p>How much missing data is there?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Number of null in label: &#123;&#125;"</span>.format(fullCorpus[<span class="string">"label"</span>].isnull().sum()))</span><br><span class="line">print(<span class="string">"Number of null in text: &#123;&#125;"</span>.format(fullCorpus[<span class="string">"body_text"</span>].isnull().sum()))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Number of null <span class="keyword">in</span> label: <span class="number">0</span></span><br><span class="line">Number of null <span class="keyword">in</span> text: <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>This helped us better understand our data once we actually got it read in. The insights pulled from this very basic exploration will help dictate how we approach the rest of our data cleaning and model building.</p>
<h1 id="regular-expressions"><a class="markdownIt-Anchor" href="#regular-expressions"></a> Regular Expressions</h1>
<h2 id="introduction-to-regular-expression"><a class="markdownIt-Anchor" href="#introduction-to-regular-expression"></a> Introduction to Regular Expression</h2>
<p>A regular expression, or a regex for short is a text string used for describing a certain search pattern.  So if you’re familiar with wildcards for search, like if you wanted to search for any CSV file on your computer using *.csv, this is basically just a supercharged version of that.  Regular expressions can take various forms.</p>
<p>To give a very quick example of what it means, the regular expression ‘nlp’ will just search for the explicit “nlp” string within some other string.  This isn’t so much a search pattern as it is an explicit command for what we want to find.  So if it was, “I love nlp” then this search pattern would just capture and return “nlp.”  Another way to identify the “nlp” string would be to use the expression ‘[j-q]’.  And this will just search for all single characters between ‘j’ and ‘q’ in whatever text we’re looking at, but this will search for all characters between ‘j’ and ‘q’, not just ‘n’, ‘l’, and ‘p’.  The other consideration here is that this will only return single characters at a time.  So this would return ‘n’ and then ‘l’ and then ‘p’ and also whatever other characters between ‘j’ and ‘q’ in your text string.  This isn’t usually what we’re looking for.  So we can solve that issue of only returning a single character by simply placing a plus sign outside of our brackets like ‘[j-q]+’.  What that will tell Python is that it can search for strings longer than one character.  So what this will look for is any character between ‘j’ and ‘q’, just with the added flexibility of returning strings of multiple characters together that are between ‘j’ and ‘q’.  Switching gears a little bit, ‘[0-9]+’ will return all numbers with the flexibility of returning sequences of more than one number.  So if there’s a year, like 2017, it will return the full year, rather than each number individually.  Then lastly, to combine these two concepts, ‘[j-q0-9]+’ will search for sequences of characters between ‘j’ and ‘q’, or numbers between 0 and 9.  So if you had a course name that was “nlp2017” without any spaces, then it would return that full string, but if you had “nlp 2017” with a space in between them, then that would return them as two separate sequences.  This is just five very quick examples, but there is literally an infinite number of patters that you could come up with.</p>
<p>Regex give you the power and flexibility to search for almost any kind of pattern you could imagine.  The examples are useful, but why do we actually care about this?  Regexes are particularly useful when dealing with text data because a lot of the data is unstructured, where you need to be able to use these patterns to try to create some structure within the document.  For instance, you could use a regex to identify the white space between words or tokens, or even let Python know how to split up a certain sentence.  Another use is to identify delimiters between columns or end-of-line escape characters that indicate the end of one line and the beginning of another like we saw in our SMS Spam Collection dataset.  They can also be used to remove punctuation or numbers, clean HTML tags, or just identify some random patterns that you’re interested in.  A few examples of regular use cases might be confirming passwords that meet some criteria.  So maybe a company requires one capital letter, one lower case and one special character in their passwords.  You can create a regex to confirm that each new password created matches that criteria.  Then the last three all kind of fall under the same broad category of searching for a certain pattern, like filenames, so find all the CSV’s that meet this criteria, or some portion of a URL, so maybe it’s whatever follows .com or .org, or scraping for key information from a larger document like package version numbers from a technical report.</p>
<h2 id="how-to-use-regular-expressions"><a class="markdownIt-Anchor" href="#how-to-use-regular-expressions"></a> How to Use Regular Expressions</h2>
<p>The primary reason that we’re talking about regex is in order to tokenize sentences or split a sentence into a list of words so that Python can understand what it needs to be looking at.  Right now, Python just sees a string of characters, so we need to tell it what to focus on, and how to organize those characters.  For our machine learning model, Python will need to split the string into what we call tokens, or words, so that the model can learn how those tokens relate to the corresponse variable.</p>
<p>Python’s <code>re</code> package is the most commonly used regex resource. More details can be found <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">here</a>.</p>
<p>Import <code>re</code> package and define 3 sentences:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">re_test = <span class="string">'This is a made up string to test 2 different regex methods'</span></span><br><span class="line">re_test_messy = <span class="string">'This      is a made up     string to test 2    different regex methods'</span></span><br><span class="line">re_test_messy1 = <span class="string">'This-is-a-made/up.string*to&gt;&gt;&gt;&gt;test----2""""""different~regex-methods'</span></span><br></pre></td></tr></table></figure>
<p>Splitting a sentence into a list of words:</p>
<p>First Sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\s"</span>, re_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>Second Sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split on a space to find words</span></span><br><span class="line">re.split(<span class="string">"\s"</span>, re_test_messy)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>You can see that the above code doesn’t work well if there are more spaces between words.  The following code can solve this problem:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split on maybe more than one spaces to find words</span></span><br><span class="line">re.split(<span class="string">"\s+"</span>, re_test_messy)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>Let’s try the third sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\s+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This-is-a-made/up.string*to&gt;&gt;&gt;&gt;test----2""""""different~regex-methods'</span>]</span><br></pre></td></tr></table></figure>
<p>As you can see that the code above only looks for spaces but the third sentance contains lots of special characters so the code will not work on this sentance.  However, the following code fix this problem by only searching non word characters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\W+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>There are other two options that can search all useful words instead of searching for non words but give the same result in the end.  The following 3 lines of code will show you how it does.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"\S+"</span>, re_test)</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"\S+"</span>, re_test_messy)</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"\w+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p><em><strong>Notice: uppercase usually means the opposite of lowercase that lowercase searches for all specified instances but uppercase will search others instead of the specified instances</strong></em></p>
<p>So that’s how we can use two different methods from the re package along with several different regexes to properly tokenize messy sentences. Now that we’ve covered some of the basic regex usage for the purpose of tokenizing, there are a few takeaways to keep in mind. There are two methods from the re package that can be used for tokenizing. findall() will search for the actual words while ignoring the things that separate the words, while split() will search for the characters that split the words while ignoring the actual words themselves. And the regexes that are most useful for tokenizing, keep in mind that anything using a W is based on words, while anything with an S is based on white spaces. In our daily work, it’s much more common to be using the W regex because it allows the flexibility for words to be separated by spaces, or special characters. But having an understanding of what the S offers you is a nice tool to hold in your back pocket.</p>
<h2 id="regular-expression-replacements"><a class="markdownIt-Anchor" href="#regular-expression-replacements"></a> Regular Expression Replacements</h2>
<p>The following sentances are the examples that we need to capture a section of the sentance and replace with other words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pep8_test = <span class="string">'I try to follow PEP8 guidelines'</span></span><br><span class="line">pep7_test = <span class="string">'I try to follow PEP7 guidelines'</span></span><br><span class="line">peep8_test = <span class="string">'I try to follow PEEP8 guidelines'</span></span><br></pre></td></tr></table></figure>
<p>We need to replace PEP8, PEP7 and PEEP8 with PEP8 Python Styleguide.  The following code is the experiment of this replacement:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"[a-z]+"</span>, pep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'try'</span>, <span class="string">'to'</span>, <span class="string">'follow'</span>, <span class="string">'guidelines'</span>]</span><br></pre></td></tr></table></figure>
<p>The above code is to find out all lower case words, but we need to find the uppercase ones, so we are going to change it like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"[A-Z]+"</span>, pep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'I'</span>, <span class="string">'PEP'</span>]</span><br></pre></td></tr></table></figure>
<p>As you can notice from the output, it captures all uppercase word.  However, we also need digits in the end to get the output of PEP8, PEP7 and PEEP8.  Therefore, we simply put [0-9]+ to make it possible.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"[A-Z]+[0-9]+"</span>, peep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'PEEP8'</span>]</span><br></pre></td></tr></table></figure>
<p>Here is our final searching result, but we need to replace this section, how can we do?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(<span class="string">"[A-Z]+[0-9]+"</span>, <span class="string">"PEP8 Python Styleguide"</span>, peep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'I try to follow PEP8 Python Styleguide guidelines'</span></span><br></pre></td></tr></table></figure>
<p>You can see that the sub function can help us replacing our search section with another defined section.</p>
<p>Now this regex certainly isn’t perfect, you can imagine a scenario that it would miss, for instance if there’s a space between pep and 8 or if it was lowercase, it would miss both of those, so you’d likely need to spend some time refining your regex.  However the point is to illustrate how you can use regex and give a practical example of a case that you might use it in.  So now up to this point we’ve explored three different regex methods with some practical application of how you might use it.  But there are a lot of other regex methods within this repackage.  Few are listed below, so there’s search, match, full match, find itter, and escape.  Even more broadly, the regex that we explored are very, very simple.  They can get very complex.  And the best way to learn it is by defining your own string with the goal of identifying some substring, pull up a regex cheat sheet, and start exploring different patterns to try to identify that substring.</p>
<h3 id="other-examples-of-regex-methods"><a class="markdownIt-Anchor" href="#other-examples-of-regex-methods"></a> Other examples of regex methods</h3>
<ul>
<li>re.search()</li>
<li>re.match()</li>
<li>re.fullmatch()</li>
<li>re.finditer()</li>
<li>re.escape()</li>
</ul>
<h1 id="machine-learning-pipeline"><a class="markdownIt-Anchor" href="#machine-learning-pipeline"></a> Machine Learning Pipeline</h1>
<p>Up to this point, we’ve learned some basics of NLP and NLTK.  We’ve learned how to read in messy text, and we’ve learned how to use regular expressions to search for and manipulate that text.  Now, we’ll take a step back to understand how this all fits together in the broader machine learning pipeline before we dive into each step individually.  This section is going to introduce some new topics as well and we’ll cover each of these topics later.  This is meant only to provide the proper context for how this all fits together.</p>
<p>In a typical machine learning text pipeline, you’ll start with some document with raw text in it, like the SMS data set that we’re working with.  It’s important to note that at this stage, the computer has no idea what it’s looking at.  All it sees is a collection of characters.  It doesn’t know the word ham from the word spam.  The characters mean nothing.  It doesn’t even know a space from a number or a letter.  They’re all the same.</p>
<p>So the first thing we need to do is tokenize our text.  We did this earlier in this chapter by splitting on white space or special characters.  So you would take the sentence, “I am learning NLP,” and it would split into a list with four tokens, I and then am and then learning, and lastly NLP.  So now, instead of just seeing one long string of characters like it was seeing before, now Python will see a list with four distinct tokens, so it knows what to look at.  So now we have a list of tokens, so Python knows what to look at.</p>
<p>However, some of the words might be a little bit more important than other words.  For instance, the words the, and, of, or, appear very frequently but don’t really offer much information about the sentence itself.  These are what’s called stop words.  We took a quick look at these earlier in this post.  Typically, you will remove these words to allow Python to really focus in on the most pivotal words in our sentence.  So in the example we used previously, instead of a list with I, am, learning, NLP, once you remove stop words, now you’re just left with learning and NLP.  This still gets across the most important point of the sentence, but now you’re only looking at half the amount of tokens.  Also, the process of stemming helps Python realize that words like learn, learned and learning all have basically the same semantic meaning.  You may not think this is a big deal, and in a small sample it’s really not, but when you all of a sudden have a million text messages, and a corpus of 150,000 words, any words that you can remove to allow Python to focus on the most pivotal words can really make a big difference.</p>
<p>So now Python sees a list of tokens you care about, and the key words that we think are useful for building some kind of machine learning model.  Even though Python now knows what you care about, it still only sees characters.  It doesn’t know what learning or NLP even means.  So we have to convert it to a format that a machine learning algorithm can actually ingest and use to build a model.  This is a process called vectorizing.  It’s basically converting the text to a numeric representation of that text, where you are essentially counting the occurrences of each word in each text message using a matrix with one row per text message and one column per word.</p>
<p>So that you have this numeric matrix, you can now fit your actual machine learning model by feeding in your vectorized data along with your spam or ham labels.  The model will then learn the relationships between the words and the labels in order to train a model to make predictions on text messages that it has never seen before and determine whether they are spam or not.  There are various types of machine learning models.  It will be up to you to select a few of them to try out.  You’ll tailor your choices based on the type of input data you’re giving it, what you’re trying to predict, how much compute power you have, things like that.  You’ll typically test out a number of what’s called candidate models before selecting which model performs best.  Once you select the best model, you’ll evaluate that on a holdout test set, and this is typically a set of data that you’ll set off to the side in the very beginning for the purpose of testing your final model on it to see how your model will perform on data that it’s never seen or touched before.  If it passes this final test, then you’ll prepare to implement it within whatever framework you’re working with.  In this example, it’s an illustration of a spam filter, trying to filter out whether incoming email is spam or not.</p>
<h1 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h1>
<p>In the previous part, we put those together at a conceptual level, laying out what the full machine learning pipeline looks like.  In this part, we’re going to actually write the code to handle to cleaning portion, or the pre-processing as it’s typically referred to, of this machine learning program.  There are four steps below that you’ll see in a lot of text cleaning pipelines: removing the punctuation, tokenization, removing stop words, and lemmatizing or stemming.  We’re going to focus on the first three steps in this session, then we’ll cover lemmatizing and stemming in the next part of the post, as those are a little bit more advanced and not implemented in every pipeline.</p>
<p>Four steps of text cleaning pipeline:</p>
<ol>
<li><strong>Remove punctuation</strong></li>
<li><strong>Tokenization</strong></li>
<li><strong>Remove stopwords</strong></li>
<li>Lemmatize/Stem</li>
</ol>
<p>The following sample is our target output for all:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># What does the cleaned version look like?</span></span><br><span class="line">data_cleaned = pd.read_csv(<span class="string">"SMSSpamCollection_cleaned.tsv"</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">data_cleaned.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">[‘ive’, ‘searching’, ‘right’, ‘words’, ‘thank’, ‘breather’, ‘promise’, ‘wont’, ‘take’, ‘help’, '…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">[‘free’, ‘entry’, ‘2’, ‘wkly’, ‘comp’, ‘win’, ‘fa’, ‘cup’, ‘final’, ‘tkts’, ‘21st’, ‘may’, '2005…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">[‘nah’, ‘dont’, ‘think’, ‘goes’, ‘usf’, ‘lives’, ‘around’, ‘though’]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">[‘even’, ‘brother’, ‘like’, ‘speak’, ‘treat’, ‘like’, ‘aids’, ‘patent’]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[‘date’, ‘sunday’]</td>
</tr>
</tbody>
</table>
<h2 id="removing-punctuation"><a class="markdownIt-Anchor" href="#removing-punctuation"></a> Removing Punctuation</h2>
<p>Import data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># Set dataframe max coloumn width to 100 characters, and default is 50</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the file with the delimiter of tabs and set header to none</span></span><br><span class="line">data = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setting the header manually</span></span><br><span class="line">data.columns = [<span class="string">'label'</span>, <span class="string">'body_text'</span>]</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>Import string package and show what punctuation has:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">string.punctuation</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~'</span></span><br></pre></td></tr></table></figure>
<p>This is really helpful to allow Python to identify what we’re looking for.  The reason that we care about this is that periods, parentheses, and other punctuation look like just another character to Python.  But realistically, the period doesn’t really help pull the meaning out of a sentence.  In the following example, for us “I like NLP.”, with a period, is exactly the same as, “I like NLP”.  They mean the same thing to us, but when you give that to Python, Python says those are not equivalent things.  And Python isn’t saying “I like NLP” without a period is different than “I like NLP.”  With a period, in that they’re really close, but one has a period and one doesn’t.  To Python, these might was well be “I like NLP” versus “I hate NLP”.  It knows they’re different without any ability to understand how different they are.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"I like NLP."</span> == <span class="string">"I like NLP"</span></span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>There is the function to remove the punctuation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_punctuation</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># Check if a character is in string punctuationk, if it is not, return this charactor</span></span><br><span class="line">    <span class="comment"># and the join all characters together by using "".join</span></span><br><span class="line">    text_nopunct = <span class="string">""</span>.join(char <span class="keyword">for</span> char <span class="keyword">in</span> text <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation)</span><br><span class="line">    <span class="keyword">return</span> text_nopunct</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is going to apply our lambda function to each row of body_text</span></span><br><span class="line">data[<span class="string">"body_text_clean"</span>] = data[<span class="string">"body_text"</span>].apply(<span class="keyword">lambda</span> x : remove_punctuation(x))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
</tr>
</tbody>
</table>
<h2 id="tokenisation"><a class="markdownIt-Anchor" href="#tokenisation"></a> Tokenisation</h2>
<p>As we discussed previously, tokenizing is splitting some string or sentence into a list of words.  We learn that you have to account for extra cases in your strings, like if they’re separated by special characters or multiple spaces.  So we’ll just use what we learned in our lesson about regexes, and combine that with the approach we learned in the last lesson, where we removed punctuation by writing our own function, and then applying it to our data set using a lambda function in order to tokenize our text.</p>
<p>The following code is going to tokenise text data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenise</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># The \W+ regex, indicates that it will split wherever it sees one or more non-word characters</span></span><br><span class="line">    tokens = re.split(<span class="string">"\W+"</span>, text)</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># Similar to removing punctuation, apply the function to text data as well as lower the case</span></span><br><span class="line"><span class="comment"># because Python is case sensitive</span></span><br><span class="line">data[<span class="string">"body_text_tokenised"</span>] = data[<span class="string">"body_text_clean"</span>].apply(<span class="keyword">lambda</span> x : tokenise(x.lower()))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_clean</th>
<th style="text-align:center">body_text_tokenised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
<td style="text-align:center">[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, …</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
<td style="text-align:center">[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
<td style="text-align:center">[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
<td style="text-align:center">[i, have, a, date, on, sunday, with, will]</td>
</tr>
</tbody>
</table>
<p>Example of case sensitive:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"NLP"</span> == <span class="string">"nlp"</span></span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="removing-stop-words"><a class="markdownIt-Anchor" href="#removing-stop-words"></a> Removing Stop Words</h2>
<p>The last step in cleaning up this data is to remove stopwords.  Now we’ve discussed stopwords previously.  They are commonly-used words like the, but, if, that don’t contribute much to the meaning of a sentence.  So we want to remove them, to limit the number of tokens Python actually has to look at when building our model.  For instance, take the sentence,  I am learning NLP.  After tokenizing, it would have four tokens, I, am, learning, and NLP.  Then after removing stopwords, instead of a list with four tokens, you’re now left with just learning and NLP.  So it gets across the same message, and now, your machine learning model only has to look at half the number of tokens.</p>
<p>Get all stop words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">stopword = nltk.corpus.stopwords.words(<span class="string">"english"</span>)</span><br></pre></td></tr></table></figure>
<p>Remove all stop words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_stopwords</span><span class="params">(tokenised_list)</span>:</span></span><br><span class="line">    text = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokenised_list <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopword]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">"body_text_nostop"</span>] = data[<span class="string">"body_text_tokenised"</span>].apply(<span class="keyword">lambda</span> x : remove_stopwords(x))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_clean</th>
<th style="text-align:center">body_text_tokenised</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
<td style="text-align:center">[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, …</td>
<td style="text-align:center">[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
<td style="text-align:center">[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to…</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
<td style="text-align:center">[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
<td style="text-align:center">[i, have, a, date, on, sunday, with, will]</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
</tbody>
</table>
<p>So now we have a cleaned column that has been tokenised, we’ve removed the punctuation and we’ve removed the stopwords.  So that is a very abbreviated look at what a pre-processing pipeline looks like as you’re preparing to get your raw text into a format that a machine learning model can actually use. In the next chapter, we’ll explore some extra, slightly more advanced cleaning techniques and concepts that we can apply to our text to further help a machine learning model focus on the things that are really important.</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Zilan Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/">http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Data-Science/">Data Science    </a><a class="post-meta__tags" href="/tags/Data-Processing/">Data Processing    </a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning    </a><a class="post-meta__tags" href="/tags/AI/">AI    </a></div><div class="post_share"><div class="social-share" data-image="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpeg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpeg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/08/20/Spark-for-Machine-Learning-AI/"><img class="next_cover lozad" data-src="https://live.staticflickr.com/65535/48621537921_7855b8a711_o.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Spark for Machine Learning and AI</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/08/20/Spark-for-Machine-Learning-AI/" title="Spark for Machine Learning and AI"><img class="relatedPosts_cover lozad" data-src="https://live.staticflickr.com/65535/48621537921_7855b8a711_o.jpg"><div class="relatedPosts_title">Spark for Machine Learning and AI</div></a></div><div class="relatedPosts_item"><a href="/2019/08/19/Hadoop-Environment-Setup/" title="Hadoop Environment Setup"><img class="relatedPosts_cover lozad" data-src="https://live.staticflickr.com/65535/48621680177_6107d2deaf_o.png"><div class="relatedPosts_title">Hadoop Environment Setup</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/';
  this.page.identifier = '2019/09/02/Natural-Language-Processing/';
  this.page.title = 'Natural Language Processing (NLP)';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'hoanjinan-otoko' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Zilan Huang</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#introduction-to-the-post"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> Introduction to The Post</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#introduction-to-natural-language-processing-nlp"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> Introduction to Natural Language Processing (NLP)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#introduction-to-nltk"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> Introduction to NLTK</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#how-to-install-nltk-on-your-local-machine"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> How to install NLTK on your local machine</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#download-nltk-data"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text"> Download NLTK data</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#reading-in-text-data"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text"> Reading in Text Data</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#what-is-unstructured-data"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text"> What is Unstructured Data</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#read-semi-structured-data"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text"> Read Semi-structured data</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#exploring-the-dataset"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text"> Exploring The Dataset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#regular-expressions"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text"> Regular Expressions</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introduction-to-regular-expression"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text"> Introduction to Regular Expression</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#how-to-use-regular-expressions"><span class="toc_mobile_items-number">7.2.</span> <span class="toc_mobile_items-text"> How to Use Regular Expressions</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#regular-expression-replacements"><span class="toc_mobile_items-number">7.3.</span> <span class="toc_mobile_items-text"> Regular Expression Replacements</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#other-examples-of-regex-methods"><span class="toc_mobile_items-number">7.3.1.</span> <span class="toc_mobile_items-text"> Other examples of regex methods</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#machine-learning-pipeline"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text"> Machine Learning Pipeline</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#implementation"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text"> Implementation</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#removing-punctuation"><span class="toc_mobile_items-number">9.1.</span> <span class="toc_mobile_items-text"> Removing Punctuation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#tokenisation"><span class="toc_mobile_items-number">9.2.</span> <span class="toc_mobile_items-text"> Tokenisation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#removing-stop-words"><span class="toc_mobile_items-number">9.3.</span> <span class="toc_mobile_items-text"> Removing Stop Words</span></a></li></ol></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" data-click="false"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":100,"vOffset":-10},"mobile":{"show":true},"log":false,"tagMode":false});</script></body></html>