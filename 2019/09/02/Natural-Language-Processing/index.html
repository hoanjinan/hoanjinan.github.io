<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Natural Language Processing (NLP) | hoanjinan_otoko</title><meta name="description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatising, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta name="keywords" content="Data Science,Data Processing,Machine Learning,AI"><meta name="author" content="Zilan Huang"><meta name="copyright" content="Zilan Huang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Natural Language Processing (NLP)"><meta name="twitter:description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatising, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta name="twitter:image" content="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Natural Language Processing (NLP)"><meta property="og:url" content="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/"><meta property="og:site_name" content="hoanjinan_otoko"><meta property="og:description" content="Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you're trying to type?  In this post, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatising, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."><meta property="og:image" content="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="Spark for Machine Learning and AI" href="http://hoanjinan.github.io/2019/08/20/Spark-for-Machine-Learning-AI/"><link rel="manifest" href="/img/pwa/manifest.json"><meta name="theme-color" content="#fff"><meta name="msapplication-TileColor" content="#fff"><link rel="apple-touch-icon" sizes="180x180" href="/img/pwa/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/pwa/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/pwa/16.png"><link rel="mask-icon" href="/img/pwa/safari-pinned-tab.svg" color="#5bbad5"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a29c9ae6807bf668430dda20bfdf3ac5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-146505841-1', 'auto');
ga('send', 'pageview');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text"> Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-the-post"><span class="toc-number">1.1.</span> <span class="toc-text"> Introduction to The Post</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-natural-language-processing-nlp"><span class="toc-number">1.2.</span> <span class="toc-text"> Introduction to Natural Language Processing (NLP)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-nltk"><span class="toc-number">1.3.</span> <span class="toc-text"> Introduction to NLTK</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp-basics"><span class="toc-number">2.</span> <span class="toc-text"> NLP Basics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-install-nltk-on-local-machine"><span class="toc-number">2.1.</span> <span class="toc-text"> How to install NLTK on local machine</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#download-nltk-data"><span class="toc-number">2.1.1.</span> <span class="toc-text"> Download NLTK data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reading-in-text-data"><span class="toc-number">2.2.</span> <span class="toc-text"> Reading in Text Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-unstructured-data"><span class="toc-number">2.2.1.</span> <span class="toc-text"> What is Unstructured Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#read-semi-structured-data"><span class="toc-number">2.2.2.</span> <span class="toc-text"> Read Semi-structured data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#exploring-the-dataset"><span class="toc-number">2.3.</span> <span class="toc-text"> Exploring The Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regular-expressions"><span class="toc-number">2.4.</span> <span class="toc-text"> Regular Expressions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-to-regular-expression"><span class="toc-number">2.4.1.</span> <span class="toc-text"> Introduction to Regular Expression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-regular-expressions"><span class="toc-number">2.4.2.</span> <span class="toc-text"> How to Use Regular Expressions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regular-expression-replacements"><span class="toc-number">2.4.3.</span> <span class="toc-text"> Regular Expression Replacements</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#other-examples-of-regex-methods"><span class="toc-number">2.4.3.1.</span> <span class="toc-text"> Other examples of regex methods</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#machine-learning-pipeline"><span class="toc-number">2.5.</span> <span class="toc-text"> Machine Learning Pipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation"><span class="toc-number">2.6.</span> <span class="toc-text"> Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#removing-punctuation"><span class="toc-number">2.6.1.</span> <span class="toc-text"> Removing Punctuation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tokenisation"><span class="toc-number">2.6.2.</span> <span class="toc-text"> Tokenisation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#removing-stop-words"><span class="toc-number">2.6.3.</span> <span class="toc-text"> Removing Stop Words</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#supplemental-data-cleaning"><span class="toc-number">3.</span> <span class="toc-text"> Supplemental Data Cleaning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introducting-stemming"><span class="toc-number">3.1.</span> <span class="toc-text"> Introducting Stemming</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#using-stemming"><span class="toc-number">3.2.</span> <span class="toc-text"> Using Stemming</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introducting-lemmatising"><span class="toc-number">3.3.</span> <span class="toc-text"> Introducting Lemmatising</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#using-lemmatising"><span class="toc-number">3.4.</span> <span class="toc-text"> Using Lemmatising</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#test-out-wordnet-lemmatiser-read-more-about-wordnet-here"><span class="toc-number">3.4.1.</span> <span class="toc-text"> Test out WordNet lemmatiser (read more about WordNet here)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#comparation-between-lemmatiser-and-stemmer"><span class="toc-number">3.4.2.</span> <span class="toc-text"> Comparation between Lemmatiser and Stemmer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#first-example"><span class="toc-number">3.4.2.1.</span> <span class="toc-text"> First Example</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#second-example"><span class="toc-number">3.4.2.2.</span> <span class="toc-text"> Second Example</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#apply-lammatiser-to-sms-spam-collection-data"><span class="toc-number">3.4.3.</span> <span class="toc-text"> Apply lammatiser to SMS Spam Collection Data</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vectorising-raw-data"><span class="toc-number">4.</span> <span class="toc-text"> Vectorising Raw Data</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introducing-vectorising"><span class="toc-number">4.1.</span> <span class="toc-text"> Introducing Vectorising</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#count-vectorisation"><span class="toc-number">4.2.</span> <span class="toc-text"> Count Vectorisation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#n-gram-vectorising"><span class="toc-number">4.3.</span> <span class="toc-text"> N-gram Vectorising</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inverse-document-frequency-weighting"><span class="toc-number">4.4.</span> <span class="toc-text"> Inverse Document Frequency Weighting</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://live.staticflickr.com/65535/48621182418_494606b3cf_o.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">hoanjinan_otoko</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Natural Language Processing (NLP)</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-09-02<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-09-06</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Tutorial/">Tutorial</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Tutorial/Data-Processing/">Data Processing</a></span><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">10.6k</span><span class="post-meta__separator">|</span><span>Reading time: 65 min</span><span class="post-meta__separator">|</span><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h1>
<h2 id="introduction-to-the-post"><a class="markdownIt-Anchor" href="#introduction-to-the-post"></a> Introduction to The Post</h2>
<p>Have you ever wondered how your email filters out spam messages?  Or maybe how autocorrect on your phone knows what you’re trying to type?  In this post, we’ll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data.  Then the post will cover some of the more advanced topics like lemmatising, stemming, and vectorizing the data.  In other words, converting it from text into a numeric matrix.  The post do this with a focus on preparing the data to build a machine learning classifier on top of it.  The post will also learn how to build two different types of machine learning models, while thoroughly testing and evaluating different variations of those models.  We’ll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems.</p>
<h2 id="introduction-to-natural-language-processing-nlp"><a class="markdownIt-Anchor" href="#introduction-to-natural-language-processing-nlp"></a> Introduction to Natural Language Processing (NLP)</h2>
<p>Natural language processing is a field concerned with the ability of a computer to understand, analyze, manipulate, and potentially generate human language. By human language, we’re simply referring to any language used for everyday communication.  This can be English, Spanish, French, anything like that.  Now it’s worth noting that Python doesn’t naturally know what any given word means.  All it will see is a string of characters.  For instance, it has no idea what natural actually means.  It sees that it’s seven characters long, but the individual characters don’t mean anything to Python and certainly the collection of those characters together don’t mean anything, either.  So we know that, what an N is, what an A is, and we know that together, those seven characters makes up the word natural, and we know what that means.  So NLP is the field of getting the computer to understand what naturally actually signifies, and from there we can get into the manipulation or potentially even generation of that human language.</p>
<p>You probably experience natural language processing on a daily basis.  They may not really even know it.  So here are a few examples that you may see on a day to day basis.  The first would be a spam filter, so this is just where your email server is determining whether an incoming email is spam or not, based on the content of the body, the subject, and maybe the email domain.  The second is auto-complete, where Google is basically predicting what you’re interested in searching for based on what you’ve already entered and what others commonly search for with those same phrases.  So if I search for natural language processing, it knows that many other people are interested in learning NLP with Python, or learning it through a course, or looking for jobs related to natural language processing.  So it can auto-complete your search for you.  The last is auto-correct, where say iPhone is trying to help you correct a misspelling. It shows how auto-correct has actually evolved over time and continues to evolve and learn by upgrading the operating system.  So with iOS 6, if you’re trying to say, “I’ll be ill tomorrow,” It wouldn’t necessarily correct I’ll be I’ll tomorrow until iOS 7, where it actually corrects, it auto-completes tomorrow and corrects I’ll into ill.  So it’ll correctly send as I’ll be ill tomorrow.  So that just kind of shows how NLP is still evolving and how a system like iOS is still kind of learning what natural language even means.</p>
<p>Now NLP is a very broad umbrella that encompasses many topics. A few of those might be sentiment analysis, topic modeling, text classification, and sentence segmentation or part-or-speech tagging.  The core component of natural language processing is extracting all the information from a block of text that is relevant to a computer understanding the language.  This is task specific, as well.  Different information is relevant for a sentiment analysis task than is relevant for a topic modeling task. So that’s a very quick introduction into what natural language processing is.</p>
<h2 id="introduction-to-nltk"><a class="markdownIt-Anchor" href="#introduction-to-nltk"></a> Introduction to NLTK</h2>
<p>The natural language toolkit is the most utilized package for handling natural language processing tasks in Python.  Usually called NLTK for short, it is a suite of open-source tools originally created in 2001 at the University of Pennsylvania for the purpose of making building NLP processes in Python easier.  This package has been expanded through the extensive contributions of open-source users in the years since its original development.  NLTK is great because it basically provides a jumpstart to building any NLP process by giving you the basic tools that you can then chain together to accomplish your goal rather than having to build all those tools from scratch and a lot of tools are packaged into NLTK.</p>
<h1 id="nlp-basics"><a class="markdownIt-Anchor" href="#nlp-basics"></a> NLP Basics</h1>
<h2 id="how-to-install-nltk-on-local-machine"><a class="markdownIt-Anchor" href="#how-to-install-nltk-on-local-machine"></a> How to install NLTK on local machine</h2>
<p>Both sets of instructions below assume you already have Python installed. These instructions are taken directly from <a href="http://www.nltk.org/install.html" target="_blank" rel="noopener">http://www.nltk.org/install.html</a>.</p>
<p><strong>Mac/Unix</strong></p>
<p>From the terminal:</p>
<ol>
<li>Install NLTK: run <code>pip install -U nltk</code></li>
<li>Test installation: run <code>python</code> then type <code>import nltk</code></li>
</ol>
<p><strong>Windows</strong></p>
<ol>
<li>Install NLTK: <a href="http://pypi.python.org/pypi/nltk" target="_blank" rel="noopener">http://pypi.python.org/pypi/nltk</a></li>
<li>Test installation: <code>Start&gt;Python35</code>, then type <code>import nltk</code></li>
</ol>
<h3 id="download-nltk-data"><a class="markdownIt-Anchor" href="#download-nltk-data"></a> Download NLTK data</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download()</span><br></pre></td></tr></table></figure>
<p>The following images will show the downloader of NLTK:</p>
<p><img alt="NLTK All Packages Uninstalled" data-src="/img/NLP/NLTK_Uninstalled.png" class="lozad"></p>
<p>In above image, select ‘all packages’ and click ‘download’ button to start installing all NLTK packages.</p>
<p><img alt="NLTK All Packages Installed" data-src="/img/NLP/NLTK_Installed.png" class="lozad"></p>
<p>The above image shows that all NLTK packages have been installed.</p>
<p>Small example of using NLTK package:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stopwords.words(<span class="string">"english"</span>)[<span class="number">0</span>:<span class="number">500</span>:<span class="number">25</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'i'</span>, <span class="string">'herself'</span>, <span class="string">'been'</span>, <span class="string">'with'</span>, <span class="string">'here'</span>, <span class="string">'very'</span>, <span class="string">'doesn'</span>, <span class="string">'won'</span>]</span><br></pre></td></tr></table></figure>
<h2 id="reading-in-text-data"><a class="markdownIt-Anchor" href="#reading-in-text-data"></a> Reading in Text Data</h2>
<h3 id="what-is-unstructured-data"><a class="markdownIt-Anchor" href="#what-is-unstructured-data"></a> What is Unstructured Data</h3>
<p>Unstructured data could mean that it’s binary data, it could mean no delimiters, or it could mean no indications of any rows.  A few examples might be an email, PDF file, social media post, these may just get dumped into a file with no indication of where, maybe, a subject of an email ends and the body of the email begins, or even where one email ends and the next begins.  It could also get cluttered by things like HTML tags and it can get really messy.  It’s important to note that Python is pretty smart, but ultimately, unless it’s told otherwise, it basically sees everything as a string of characters. It needs to be told what those characters mean.</p>
<h3 id="read-semi-structured-data"><a class="markdownIt-Anchor" href="#read-semi-structured-data"></a> Read Semi-structured data</h3>
<p>The following image presents a semi-structured SMS data:</p>
<p><img alt="Semi-structured SMS data sample" data-src="/img/NLP/SMS.png" class="lozad"></p>
<p>This dataset is a collection of text messages, each with a label of either spam or ham.  It’s not a clean CSV file, but it’s not terribly unstructured, either.  Each row has a distinct text message and a distinct label as either spam or ham. So, in the context of text datasets, this is actually pretty well structured, so this shouldn’t be too difficult.</p>
<p>Reading the data and print it out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Read in the raw text</span></span><br><span class="line">rawData = open(<span class="string">"SMSSpamCollection.tsv"</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the raw data</span></span><br><span class="line">rawData[<span class="number">0</span>:<span class="number">500</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\nspam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's\nham\tNah I don't think he goes to usf, he lives around here though\nham\tEven my brother is not like to speak with me. They treat me like aid"</span></span><br></pre></td></tr></table></figure>
<p>You could see that it’s just basically a block of text, and you’ll see that you have these \t and these \n separators. The \t’s are between the labels and the text message bodies, and the \n’s are typically at the end of those lines.</p>
<p>The following code is going to replace \n with \t and then split this into a list:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parsedData = rawData.replace(<span class="string">"\t"</span>, <span class="string">"\n"</span>).split(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">parsedData[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>,</span><br><span class="line"> <span class="string">"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."</span>,</span><br><span class="line"> <span class="string">'spam'</span>,</span><br><span class="line"> <span class="string">"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"</span>,</span><br><span class="line"> <span class="string">'ham'</span>]</span><br></pre></td></tr></table></figure>
<p>Split the label and the text into lists:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labelList = parsedData[<span class="number">0</span>::<span class="number">2</span>]</span><br><span class="line">textList = parsedData[<span class="number">1</span>::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>Print the results:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(labelList[<span class="number">0</span>:<span class="number">5</span>])</span><br><span class="line">print(textList[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>, <span class="string">'spam'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>]</span><br><span class="line">[<span class="string">"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."</span>, <span class="string">"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"</span>, <span class="string">"Nah I don't think he goes to usf, he lives around here though"</span>, <span class="string">'Even my brother is not like to speak with me. They treat me like aids patent.'</span>, <span class="string">'I HAVE A DATE ON SUNDAY WITH WILL!!'</span>]</span><br></pre></td></tr></table></figure>
<p>Combine both lists and put them into pandas dataframe:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fullCorpus = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"label"</span>: labelList,</span><br><span class="line">    <span class="string">"body_list"</span>: textList</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: arrays must all be same length</span><br></pre></td></tr></table></figure>
<p>Spot the error:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(len(labelList))</span><br><span class="line">print(len(textList))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5571</span></span><br><span class="line"><span class="number">5570</span></span><br></pre></td></tr></table></figure>
<p>Print the last 5 values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(labelList[<span class="number">-5</span>:])</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">'ham'</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure>
<p>Correction:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fullCorpus = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"label"</span>: labelList[:<span class="number">-1</span>],</span><br><span class="line">    <span class="string">"body_list"</span>: textList</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>Read the file using pandas:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep = <span class="string">"\t"</span>, header = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">dataset.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<h2 id="exploring-the-dataset"><a class="markdownIt-Anchor" href="#exploring-the-dataset"></a> Exploring The Dataset</h2>
<p>Before diving into any in-depth analysis, data cleaning or model building, we want to do some very high-level exploration of our data to understand what we’re working with.  So we might ask questions like what is the shape of our data, how many ham or spam are in our data set, and are there any missing values.  So this will inform the decisions that we make as we move forward.</p>
<p>Read the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fullCorpus = pd.read_csv(<span class="string">'SMSSpamCollection.tsv'</span>, sep=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">fullCorpus.columns = [<span class="string">"label"</span>, <span class="string">"body_text"</span>]</span><br><span class="line"></span><br><span class="line">fullCorpus.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to tha…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup fina…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives aro…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. …</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>What is the shape of the dataset?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Input data has &#123;&#125; rows and &#123;&#125; columns"</span>.format(len(fullCorpus), len(fullCorpus.columns)))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Input data has <span class="number">5568</span> rows <span class="keyword">and</span> <span class="number">2</span> columns</span><br></pre></td></tr></table></figure>
<p>How many spam/ham are there?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Out of &#123;&#125; rows, &#123;&#125; are spam, &#123;&#125; are ham"</span>.format(len(fullCorpus),</span><br><span class="line">                                                       len(fullCorpus[fullCorpus[<span class="string">"label"</span>] == <span class="string">"spam"</span>]),</span><br><span class="line">                                                       len(fullCorpus[fullCorpus[<span class="string">"label"</span>] == <span class="string">"ham"</span>])))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Out of <span class="number">5568</span> rows, <span class="number">746</span> are spam, <span class="number">4822</span> are ham</span><br></pre></td></tr></table></figure>
<p>How much missing data is there?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Number of null in label: &#123;&#125;"</span>.format(fullCorpus[<span class="string">"label"</span>].isnull().sum()))</span><br><span class="line">print(<span class="string">"Number of null in text: &#123;&#125;"</span>.format(fullCorpus[<span class="string">"body_text"</span>].isnull().sum()))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Number of null <span class="keyword">in</span> label: <span class="number">0</span></span><br><span class="line">Number of null <span class="keyword">in</span> text: <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>This helped us better understand our data once we actually got it read in. The insights pulled from this very basic exploration will help dictate how we approach the rest of our data cleaning and model building.</p>
<h2 id="regular-expressions"><a class="markdownIt-Anchor" href="#regular-expressions"></a> Regular Expressions</h2>
<h3 id="introduction-to-regular-expression"><a class="markdownIt-Anchor" href="#introduction-to-regular-expression"></a> Introduction to Regular Expression</h3>
<p>A regular expression, or a regex for short is a text string used for describing a certain search pattern.  So if you’re familiar with wildcards for search, like if you wanted to search for any CSV file on your computer using *.csv, this is basically just a supercharged version of that.  Regular expressions can take various forms.</p>
<p>To give a very quick example of what it means, the regular expression ‘nlp’ will just search for the explicit “nlp” string within some other string.  This isn’t so much a search pattern as it is an explicit command for what we want to find.  So if it was, “I love nlp” then this search pattern would just capture and return “nlp.”  Another way to identify the “nlp” string would be to use the expression ‘[j-q]’.  And this will just search for all single characters between ‘j’ and ‘q’ in whatever text we’re looking at, but this will search for all characters between ‘j’ and ‘q’, not just ‘n’, ‘l’, and ‘p’.  The other consideration here is that this will only return single characters at a time.  So this would return ‘n’ and then ‘l’ and then ‘p’ and also whatever other characters between ‘j’ and ‘q’ in your text string.  This isn’t usually what we’re looking for.  So we can solve that issue of only returning a single character by simply placing a plus sign outside of our brackets like ‘[j-q]+’.  What that will tell Python is that it can search for strings longer than one character.  So what this will look for is any character between ‘j’ and ‘q’, just with the added flexibility of returning strings of multiple characters together that are between ‘j’ and ‘q’.  Switching gears a little bit, ‘[0-9]+’ will return all numbers with the flexibility of returning sequences of more than one number.  So if there’s a year, like 2017, it will return the full year, rather than each number individually.  Then lastly, to combine these two concepts, ‘[j-q0-9]+’ will search for sequences of characters between ‘j’ and ‘q’, or numbers between 0 and 9.  So if you had a course name that was “nlp2017” without any spaces, then it would return that full string, but if you had “nlp 2017” with a space in between them, then that would return them as two separate sequences.  This is just five very quick examples, but there is literally an infinite number of patters that you could come up with.</p>
<p>Regex give you the power and flexibility to search for almost any kind of pattern you could imagine.  The examples are useful, but why do we actually care about this?  Regexes are particularly useful when dealing with text data because a lot of the data is unstructured, where you need to be able to use these patterns to try to create some structure within the document.  For instance, you could use a regex to identify the white space between words or tokens, or even let Python know how to split up a certain sentence.  Another use is to identify delimiters between columns or end-of-line escape characters that indicate the end of one line and the beginning of another like we saw in our SMS Spam Collection dataset.  They can also be used to remove punctuation or numbers, clean HTML tags, or just identify some random patterns that you’re interested in.  A few examples of regular use cases might be confirming passwords that meet some criteria.  So maybe a company requires one capital letter, one lower case and one special character in their passwords.  You can create a regex to confirm that each new password created matches that criteria.  Then the last three all kind of fall under the same broad category of searching for a certain pattern, like filenames, so find all the CSV’s that meet this criteria, or some portion of a URL, so maybe it’s whatever follows .com or .org, or scraping for key information from a larger document like package version numbers from a technical report.</p>
<h3 id="how-to-use-regular-expressions"><a class="markdownIt-Anchor" href="#how-to-use-regular-expressions"></a> How to Use Regular Expressions</h3>
<p>The primary reason that we’re talking about regex is in order to tokenize sentences or split a sentence into a list of words so that Python can understand what it needs to be looking at.  Right now, Python just sees a string of characters, so we need to tell it what to focus on, and how to organize those characters.  For our machine learning model, Python will need to split the string into what we call tokens, or words, so that the model can learn how those tokens relate to the corresponse variable.</p>
<p>Python’s <code>re</code> package is the most commonly used regex resource. More details can be found <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">here</a>.</p>
<p>Import <code>re</code> package and define 3 sentences:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">re_test = <span class="string">'This is a made up string to test 2 different regex methods'</span></span><br><span class="line">re_test_messy = <span class="string">'This      is a made up     string to test 2    different regex methods'</span></span><br><span class="line">re_test_messy1 = <span class="string">'This-is-a-made/up.string*to&gt;&gt;&gt;&gt;test----2""""""different~regex-methods'</span></span><br></pre></td></tr></table></figure>
<p>Splitting a sentence into a list of words:</p>
<p>First Sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\s"</span>, re_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>Second Sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split on a space to find words</span></span><br><span class="line">re.split(<span class="string">"\s"</span>, re_test_messy)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">''</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>You can see that the above code doesn’t work well if there are more spaces between words.  The following code can solve this problem:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split on maybe more than one spaces to find words</span></span><br><span class="line">re.split(<span class="string">"\s+"</span>, re_test_messy)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>Let’s try the third sentance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\s+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This-is-a-made/up.string*to&gt;&gt;&gt;&gt;test----2""""""different~regex-methods'</span>]</span><br></pre></td></tr></table></figure>
<p>As you can see that the code above only looks for spaces but the third sentance contains lots of special characters so the code will not work on this sentance.  However, the following code fix this problem by only searching non word characters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">"\W+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This'</span>,</span><br><span class="line"> <span class="string">'is'</span>,</span><br><span class="line"> <span class="string">'a'</span>,</span><br><span class="line"> <span class="string">'made'</span>,</span><br><span class="line"> <span class="string">'up'</span>,</span><br><span class="line"> <span class="string">'string'</span>,</span><br><span class="line"> <span class="string">'to'</span>,</span><br><span class="line"> <span class="string">'test'</span>,</span><br><span class="line"> <span class="string">'2'</span>,</span><br><span class="line"> <span class="string">'different'</span>,</span><br><span class="line"> <span class="string">'regex'</span>,</span><br><span class="line"> <span class="string">'methods'</span>]</span><br></pre></td></tr></table></figure>
<p>There are other two options that can search all useful words instead of searching for non words but give the same result in the end.  The following 3 lines of code will show you how it does.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"\S+"</span>, re_test)</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"\S+"</span>, re_test_messy)</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"\w+"</span>, re_test_messy1)</span><br></pre></td></tr></table></figure>
<p><em><strong>Notice: uppercase usually means the opposite of lowercase that lowercase searches for all specified instances but uppercase will search others instead of the specified instances</strong></em></p>
<p>So that’s how we can use two different methods from the re package along with several different regexes to properly tokenize messy sentences. Now that we’ve covered some of the basic regex usage for the purpose of tokenizing, there are a few takeaways to keep in mind. There are two methods from the re package that can be used for tokenizing. findall() will search for the actual words while ignoring the things that separate the words, while split() will search for the characters that split the words while ignoring the actual words themselves. And the regexes that are most useful for tokenizing, keep in mind that anything using a W is based on words, while anything with an S is based on white spaces. In our daily work, it’s much more common to be using the W regex because it allows the flexibility for words to be separated by spaces, or special characters. But having an understanding of what the S offers you is a nice tool to hold in your back pocket.</p>
<h3 id="regular-expression-replacements"><a class="markdownIt-Anchor" href="#regular-expression-replacements"></a> Regular Expression Replacements</h3>
<p>The following sentances are the examples that we need to capture a section of the sentance and replace with other words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pep8_test = <span class="string">'I try to follow PEP8 guidelines'</span></span><br><span class="line">pep7_test = <span class="string">'I try to follow PEP7 guidelines'</span></span><br><span class="line">peep8_test = <span class="string">'I try to follow PEEP8 guidelines'</span></span><br></pre></td></tr></table></figure>
<p>We need to replace PEP8, PEP7 and PEEP8 with PEP8 Python Styleguide.  The following code is the experiment of this replacement:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">re.findall(<span class="string">"[a-z]+"</span>, pep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'try'</span>, <span class="string">'to'</span>, <span class="string">'follow'</span>, <span class="string">'guidelines'</span>]</span><br></pre></td></tr></table></figure>
<p>The above code is to find out all lower case words, but we need to find the uppercase ones, so we are going to change it like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"[A-Z]+"</span>, pep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'I'</span>, <span class="string">'PEP'</span>]</span><br></pre></td></tr></table></figure>
<p>As you can notice from the output, it captures all uppercase word.  However, we also need digits in the end to get the output of PEP8, PEP7 and PEEP8.  Therefore, we simply put [0-9]+ to make it possible.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">"[A-Z]+[0-9]+"</span>, peep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'PEEP8'</span>]</span><br></pre></td></tr></table></figure>
<p>Here is our final searching result, but we need to replace this section, how can we do?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(<span class="string">"[A-Z]+[0-9]+"</span>, <span class="string">"PEP8 Python Styleguide"</span>, peep8_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'I try to follow PEP8 Python Styleguide guidelines'</span></span><br></pre></td></tr></table></figure>
<p>You can see that the sub function can help us replacing our search section with another defined section.</p>
<p>Now this regex certainly isn’t perfect, you can imagine a scenario that it would miss, for instance if there’s a space between pep and 8 or if it was lowercase, it would miss both of those, so you’d likely need to spend some time refining your regex.  However the point is to illustrate how you can use regex and give a practical example of a case that you might use it in.  So now up to this point we’ve explored three different regex methods with some practical application of how you might use it.  But there are a lot of other regex methods within this repackage.  Few are listed below, so there’s search, match, full match, find itter, and escape.  Even more broadly, the regex that we explored are very, very simple.  They can get very complex.  And the best way to learn it is by defining your own string with the goal of identifying some substring, pull up a regex cheat sheet, and start exploring different patterns to try to identify that substring.</p>
<h4 id="other-examples-of-regex-methods"><a class="markdownIt-Anchor" href="#other-examples-of-regex-methods"></a> Other examples of regex methods</h4>
<ul>
<li>re.search()</li>
<li>re.match()</li>
<li>re.fullmatch()</li>
<li>re.finditer()</li>
<li>re.escape()</li>
</ul>
<h2 id="machine-learning-pipeline"><a class="markdownIt-Anchor" href="#machine-learning-pipeline"></a> Machine Learning Pipeline</h2>
<p>Up to this point, we’ve learned some basics of NLP and NLTK.  We’ve learned how to read in messy text, and we’ve learned how to use regular expressions to search for and manipulate that text.  Now, we’ll take a step back to understand how this all fits together in the broader machine learning pipeline before we dive into each step individually.  This section is going to introduce some new topics as well and we’ll cover each of these topics later.  This is meant only to provide the proper context for how this all fits together.</p>
<p>In a typical machine learning text pipeline, you’ll start with some document with raw text in it, like the SMS data set that we’re working with.  It’s important to note that at this stage, the computer has no idea what it’s looking at.  All it sees is a collection of characters.  It doesn’t know the word ham from the word spam.  The characters mean nothing.  It doesn’t even know a space from a number or a letter.  They’re all the same.</p>
<p>So the first thing we need to do is tokenize our text.  We did this earlier in this chapter by splitting on white space or special characters.  So you would take the sentence, “I am learning NLP,” and it would split into a list with four tokens, I and then am and then learning, and lastly NLP.  So now, instead of just seeing one long string of characters like it was seeing before, now Python will see a list with four distinct tokens, so it knows what to look at.  So now we have a list of tokens, so Python knows what to look at.</p>
<p>However, some of the words might be a little bit more important than other words.  For instance, the words the, and, of, or, appear very frequently but don’t really offer much information about the sentence itself.  These are what’s called stop words.  We took a quick look at these earlier in this post.  Typically, you will remove these words to allow Python to really focus in on the most pivotal words in our sentence.  So in the example we used previously, instead of a list with I, am, learning, NLP, once you remove stop words, now you’re just left with learning and NLP.  This still gets across the most important point of the sentence, but now you’re only looking at half the amount of tokens.  Also, the process of stemming helps Python realize that words like learn, learned and learning all have basically the same semantic meaning.  You may not think this is a big deal, and in a small sample it’s really not, but when you all of a sudden have a million text messages, and a corpus of 150,000 words, any words that you can remove to allow Python to focus on the most pivotal words can really make a big difference.</p>
<p>So now Python sees a list of tokens you care about, and the key words that we think are useful for building some kind of machine learning model.  Even though Python now knows what you care about, it still only sees characters.  It doesn’t know what learning or NLP even means.  So we have to convert it to a format that a machine learning algorithm can actually ingest and use to build a model.  This is a process called vectorizing.  It’s basically converting the text to a numeric representation of that text, where you are essentially counting the occurrences of each word in each text message using a matrix with one row per text message and one column per word.</p>
<p>So that you have this numeric matrix, you can now fit your actual machine learning model by feeding in your vectorized data along with your spam or ham labels.  The model will then learn the relationships between the words and the labels in order to train a model to make predictions on text messages that it has never seen before and determine whether they are spam or not.  There are various types of machine learning models.  It will be up to you to select a few of them to try out.  You’ll tailor your choices based on the type of input data you’re giving it, what you’re trying to predict, how much compute power you have, things like that.  You’ll typically test out a number of what’s called candidate models before selecting which model performs best.  Once you select the best model, you’ll evaluate that on a holdout test set, and this is typically a set of data that you’ll set off to the side in the very beginning for the purpose of testing your final model on it to see how your model will perform on data that it’s never seen or touched before.  If it passes this final test, then you’ll prepare to implement it within whatever framework you’re working with.  In this example, it’s an illustration of a spam filter, trying to filter out whether incoming email is spam or not.</p>
<h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2>
<p>In the previous part, we put those together at a conceptual level, laying out what the full machine learning pipeline looks like.  In this part, we’re going to actually write the code to handle to cleaning portion, or the pre-processing as it’s typically referred to, of this machine learning program.  There are four steps below that you’ll see in a lot of text cleaning pipelines: <strong>removing the punctuation, tokenization, removing stop words, and lemmatising or stemming</strong>.  We’re going to focus on the first three steps in this session, then we’ll cover lemmatising and stemming in the next chapter of the post, as those are a little bit more advanced and not implemented in every pipeline.</p>
<p>The following sample is our target output for all:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># What does the cleaned version look like?</span></span><br><span class="line">data_cleaned = pd.read_csv(<span class="string">"SMSSpamCollection_cleaned.tsv"</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">data_cleaned.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">[‘ive’, ‘searching’, ‘right’, ‘words’, ‘thank’, ‘breather’, ‘promise’, ‘wont’, ‘take’, ‘help’, '…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">[‘free’, ‘entry’, ‘2’, ‘wkly’, ‘comp’, ‘win’, ‘fa’, ‘cup’, ‘final’, ‘tkts’, ‘21st’, ‘may’, '2005…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">[‘nah’, ‘dont’, ‘think’, ‘goes’, ‘usf’, ‘lives’, ‘around’, ‘though’]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">[‘even’, ‘brother’, ‘like’, ‘speak’, ‘treat’, ‘like’, ‘aids’, ‘patent’]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[‘date’, ‘sunday’]</td>
</tr>
</tbody>
</table>
<h3 id="removing-punctuation"><a class="markdownIt-Anchor" href="#removing-punctuation"></a> Removing Punctuation</h3>
<p>Import data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># Set dataframe max coloumn width to 100 characters, and default is 50</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the file with the delimiter of tabs and set header to none</span></span><br><span class="line">data = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setting the header manually</span></span><br><span class="line">data.columns = [<span class="string">'label'</span>, <span class="string">'body_text'</span>]</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
</tbody>
</table>
<p>Import string package and show what punctuation has:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">string.punctuation</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~'</span></span><br></pre></td></tr></table></figure>
<p>This is really helpful to allow Python to identify what we’re looking for.  The reason that we care about this is that periods, parentheses, and other punctuation look like just another character to Python.  But realistically, the period doesn’t really help pull the meaning out of a sentence.  In the following example, for us “I like NLP.”, with a period, is exactly the same as, “I like NLP”.  They mean the same thing to us, but when you give that to Python, Python says those are not equivalent things.  And Python isn’t saying “I like NLP” without a period is different than “I like NLP.”  With a period, in that they’re really close, but one has a period and one doesn’t.  To Python, these might was well be “I like NLP” versus “I hate NLP”.  It knows they’re different without any ability to understand how different they are.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"I like NLP."</span> == <span class="string">"I like NLP"</span></span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>There is the function to remove the punctuation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_punctuation</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># Check if a character is in string punctuationk, if it is not, return this charactor</span></span><br><span class="line">    <span class="comment"># and the join all characters together by using "".join</span></span><br><span class="line">    text_nopunct = <span class="string">""</span>.join(char <span class="keyword">for</span> char <span class="keyword">in</span> text <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation)</span><br><span class="line">    <span class="keyword">return</span> text_nopunct</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is going to apply our lambda function to each row of body_text</span></span><br><span class="line">data[<span class="string">"body_text_clean"</span>] = data[<span class="string">"body_text"</span>].apply(<span class="keyword">lambda</span> x : remove_punctuation(x))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
</tr>
</tbody>
</table>
<h3 id="tokenisation"><a class="markdownIt-Anchor" href="#tokenisation"></a> Tokenisation</h3>
<p>As we discussed previously, tokenizing is splitting some string or sentence into a list of words.  We learn that you have to account for extra cases in your strings, like if they’re separated by special characters or multiple spaces.  So we’ll just use what we learned in our lesson about regexes, and combine that with the approach we learned in the last lesson, where we removed punctuation by writing our own function, and then applying it to our data set using a lambda function in order to tokenize our text.</p>
<p>The following code is going to tokenise text data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenise</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># The \W+ regex, indicates that it will split wherever it sees one or more non-word characters</span></span><br><span class="line">    tokens = re.split(<span class="string">"\W+"</span>, text)</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># Similar to removing punctuation, apply the function to text data as well as lower the case</span></span><br><span class="line"><span class="comment"># because Python is case sensitive</span></span><br><span class="line">data[<span class="string">"body_text_tokenised"</span>] = data[<span class="string">"body_text_clean"</span>].apply(<span class="keyword">lambda</span> x : tokenise(x.lower()))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_clean</th>
<th style="text-align:center">body_text_tokenised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
<td style="text-align:center">[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, …</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
<td style="text-align:center">[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
<td style="text-align:center">[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
<td style="text-align:center">[i, have, a, date, on, sunday, with, will]</td>
</tr>
</tbody>
</table>
<p>Example of case sensitive:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"NLP"</span> == <span class="string">"nlp"</span></span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="removing-stop-words"><a class="markdownIt-Anchor" href="#removing-stop-words"></a> Removing Stop Words</h3>
<p>The last step in cleaning up this data is to remove stopwords.  Now we’ve discussed stopwords previously.  They are commonly-used words like the, but, if, that don’t contribute much to the meaning of a sentence.  So we want to remove them, to limit the number of tokens Python actually has to look at when building our model.  For instance, take the sentence,  I am learning NLP.  After tokenizing, it would have four tokens, I, am, learning, and NLP.  Then after removing stopwords, instead of a list with four tokens, you’re now left with just learning and NLP.  So it gets across the same message, and now, your machine learning model only has to look at half the number of tokens.</p>
<p>Get all stop words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">stopword = nltk.corpus.stopwords.words(<span class="string">"english"</span>)</span><br></pre></td></tr></table></figure>
<p>Remove all stop words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_stopwords</span><span class="params">(tokenised_list)</span>:</span></span><br><span class="line">    text = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokenised_list <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopword]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">"body_text_nostop"</span>] = data[<span class="string">"body_text_tokenised"</span>].apply(<span class="keyword">lambda</span> x : remove_stopwords(x))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_clean</th>
<th style="text-align:center">body_text_tokenised</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’ve been searching for the right words to thank you for this breather. I promise i wont take yo…</td>
<td style="text-align:center">Ive been searching for the right words to thank you for this breather I promise i wont take your…</td>
<td style="text-align:center">[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, …</td>
<td style="text-align:center">[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive …</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e…</td>
<td style="text-align:center">[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to…</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I don’t think he goes to usf, he lives around here though</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me. They treat me like aids patent.</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent</td>
<td style="text-align:center">[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL</td>
<td style="text-align:center">[i, have, a, date, on, sunday, with, will]</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
</tbody>
</table>
<p>So now we have a cleaned column that has been tokenised, we’ve removed the punctuation and we’ve removed the stopwords.  So that is a very abbreviated look at what a pre-processing pipeline looks like as you’re preparing to get your raw text into a format that a machine learning model can actually use. In the next chapter, we’ll explore some extra, slightly more advanced cleaning techniques and concepts that we can apply to our text to further help a machine learning model focus on the things that are really important.</p>
<h1 id="supplemental-data-cleaning"><a class="markdownIt-Anchor" href="#supplemental-data-cleaning"></a> Supplemental Data Cleaning</h1>
<h2 id="introducting-stemming"><a class="markdownIt-Anchor" href="#introducting-stemming"></a> Introducting Stemming</h2>
<p>The formal definition of stemming is the process of reducing inflected or derived words to their word stem or root.  More simply put, the process of stemming means often crudely chopping off the end of a word, to leave only the base.  So this means taking words with various suffixes and condensing them under the same root word.  Recall when we removed stop words, it was to reduce the number of words Python has to look at or consider.  Stemming is shooting for the same goal by reducing variations of the same root word.</p>
<p>Examples:</p>
<table>
<thead>
<tr>
<th style="text-align:center">before</th>
<th style="text-align:center">after</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Stemming / stemmed</td>
<td style="text-align:center">Stem</td>
</tr>
<tr>
<td style="text-align:center">Electricity / electrical</td>
<td style="text-align:center">Electr</td>
</tr>
<tr>
<td style="text-align:center">Berries / berry</td>
<td style="text-align:center">berri</td>
</tr>
<tr>
<td style="text-align:center">Connection / connected / connective</td>
<td style="text-align:center">Connect</td>
</tr>
</tbody>
</table>
<p>So this seems pretty useful, but stemming uses very crude rules, so it isn’t perfect.  For instance, look at meaning and meanness.  These words aren’t really all that closely related, but they’ll be both stripped down to a base of mean.  And thus Python will think that meanness and meaning are the same exact thing.  So stemmers are correct in most cases, but the trade-off with these simple rules is that it won’t always be right.</p>
<p>So this all seems interesting, but why do we really care about this?  Why does this actually help us for model building?  If Python sees grew, grow, and growing as three separate things, that means it has to keep those three separate words in memory.  Imagine every variation of every root word.  Maybe we have a thousand root words, but in our corpus, we have two thousand total words with every suffix added to the root words.  The alternative in this grew, grow, and growing example is applying the stemmer, and now it only has to know what grow means, as each variation of grow is replaced simply by grow.  So Python has to look at a lot more tokens without a stemmer and it doesn’t know that these separate tokens are even related.  So the benefits of a stemmer is <strong>it reduces the corpus of words the model is exposed to</strong>, so it’s just grow, instead of grew, grow, and growing, and <strong>it explicitly correlates words with similar meaning</strong>.  So Python could learn through the training process that we’ll discuss later, that grow, grew, and growing are similar in meaning, but it also may not, it depends on a lot of different factors.  In this case, we’re not leaving it up to Python.  We’re being explicit by replacing similar words with just one common root word.  There are a number of different types of stemmers that use various algorithms and methods to generate the stemmed version of words.  A few that are included in the NLTK package are the <strong>Porter Stemmer, the Snowball Stemmer, the Lancaster Stemmer, and a Regex-Based Stemmer</strong>. We’ll be focusing on the most popular stemmer in this list, the <strong>Porter Stemmer</strong>.</p>
<h2 id="using-stemming"><a class="markdownIt-Anchor" href="#using-stemming"></a> Using Stemming</h2>
<p>To make the use of stemming, there are two stages.  First, we’ll test out the stemmer on specific words to understand how it works.  Then we’ll apply the stemmer on the SMS spam collection data set to further clean up our data.</p>
<p>Import package:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">ps = nltk.PorterStemmer()</span><br></pre></td></tr></table></figure>
<p>First example of using the function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(ps.stem(<span class="string">"grows"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"growing"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"grow"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grow</span><br><span class="line">grow</span><br><span class="line">grow</span><br></pre></td></tr></table></figure>
<p>So that reduces them all to the proper root word of grow.  So now these three words can be treated as the same word, rather than Python seeing them as three distinctly different words.</p>
<p>We showed before how the stemmer isn’t perfect, where it stemmed both meaning and meanness down to mean, even though they don’t represent the same thing.  However, if you look at a different example that could be a little difficult, we’ll do run, running, and runner.  You could see how all three of these might be reduced down to just run. Even though the first two are actions and the last one describes a person.</p>
<p>Second example of using the function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(ps.stem(<span class="string">"run"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"running"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"runner"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">run</span><br><span class="line">run</span><br><span class="line">runner</span><br></pre></td></tr></table></figure>
<p>So the stemmer can actually tell that the first two are different than the last one in some way.  So stemmers certainly aren’t perfect, but they still do a pretty good job of identifying words that have the same meaning.</p>
<p>Let’s use it for our SMS Spam example!</p>
<p>Import packages and read the file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">data.columns = [<span class="string">'label'</span>, <span class="string">'body_text'</span>]</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
</tr>
</tbody>
</table>
<p>Clean up text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = <span class="string">""</span>.join([word <span class="keyword">for</span> word <span class="keyword">in</span> text <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation])</span><br><span class="line">    tokens = re.split(<span class="string">'\W+'</span>, text)</span><br><span class="line">    text = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokens <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">'body_text_nostop'</span>] = data[<span class="string">'body_text'</span>].apply(<span class="keyword">lambda</span> x: clean_text(x.lower()))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
<td style="text-align:center">[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr…</td>
</tr>
</tbody>
</table>
<p>We saw that the ps.stem method is what stems each word.  So the column that we’ll be operating on from this data frame is this tokenized list.  So we’ll want to iterate through the list and stem each word and then return the stemmed version back to the list.  So this should be starting to sound familiar at this point.  We’ll again write our own function using ps.stem within list comprehension in order to stem each word.</p>
<p>The following code is the way to stem the text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stemming</span><span class="params">(tokenised_text)</span>:</span></span><br><span class="line">    text = [ps.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokenised_text]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">"body_text_stemmed"</span>] = data[<span class="string">"body_text_nostop"</span>].apply(<span class="keyword">lambda</span> x : stemming(x))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
<th style="text-align:center">body_text_stemmed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
<td style="text-align:center">[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
<td style="text-align:center">[nah, dont, think, goe, usf, live, around, though]</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aid, patent]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[date, sunday]</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
<td style="text-align:center">[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr…</td>
<td style="text-align:center">[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, …</td>
</tr>
</tbody>
</table>
<p>Now it’s worth noting that the stemmer won’t do a great job with slang or abbreviations.  So it’s probably not a great fit for a text message data set.  To be noticed that entry is changed to entri with an i so it could also accommodate plural, entries.  Same thing with wkli.  Another one is on the second line, lives is reduced down to live.  So, we know what stemming represents, and how to actually apply it.  Stemming helps us reduce the corpus of words that the models are exposed to, and it explicitly correlates words with similar meaning.</p>
<h2 id="introducting-lemmatising"><a class="markdownIt-Anchor" href="#introducting-lemmatising"></a> Introducting Lemmatising</h2>
<p>The formal definition is that it’s the process of grouping together the inflected forms of a word so they can be analyzed as a single term, identified by the word’s lemma.  The lemma is the canonical form of a set of words.  For instance, type, typed, and typing would all be forms of the same lemma.  More simply put, lemmatising is using vocabulary analysis of words to remove inflectional endings and return to the dictionary form of a word.  So again, type, typed, and typing would all be simplified down to type, because that’s the root of the word.  Each variation carries the same meaning just with slightly different tense.  So you might be thinking that that sounds an awful lot like stemming, and you wouldn’t be wrong.  They are aiming to accomplish the same thing, but they are doing it in just slightly different ways.  And in practical terms, there’s an accuracy and speed trade-off that you’re making when you opt for one over the other.</p>
<p>The goal of both is to condense derived words down into their base form, to reduce the corpus of words that the model’s exposed to, and to explicitly correlate words with similar meaning.  The difference is that stemming takes a more crude approach by just chopping off the ending of a word using heuristics, without any understanding of the context in which a word is used.  Because of that, stemming may or may not return an actual word in the dictionary.  And it’s usually less accurate, but the benefit is that it’s faster because the rules are quite simple.  lemmatising leverages more informed analysis to create groups of words with similar meaning based on the context around the word, part of speech, and other factors.  lemmatisers will always return a dictionary word.  And because of the additional context it’s considered, this is typically more accurate.  But the downside is that it may be more computationally expensive.  So this is a very brief introduction into lemmatising.</p>
<h2 id="using-lemmatising"><a class="markdownIt-Anchor" href="#using-lemmatising"></a> Using Lemmatising</h2>
<p>To make the use of Lemmatising, there are two stages.  First, we’re going to test out the lemmatiser on specific words to understand how it works and then we’ll apply it on the SMS Spam Collection Data Set to further clean it up.  So the same process that we saw on the stemming notebook.  Just like we saw with stemmers, there are a few different lemmatisers as well that handle words in slightly different ways.  So we’re going to use the WordNet lemmatiser. This is probably the most popular lemmatiser.  WordNet is a collection of nouns, verbs, adjective and adverbs that are grouped together in sets of synonyms, each expressing a distinct concept.  This lemmatiser runs off of this corpus of synonyms, so given a word, it will track that word to its synonyms, and then the distinct concept that that group of words represents.</p>
<h3 id="test-out-wordnet-lemmatiser-read-more-about-wordnet-here"><a class="markdownIt-Anchor" href="#test-out-wordnet-lemmatiser-read-more-about-wordnet-here"></a> Test out WordNet lemmatiser (read more about WordNet <a href="https://wordnet.princeton.edu/" target="_blank" rel="noopener">here</a>)</h3>
<p>Import the nltk package and apply both lemmatiser and stemmer function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">wn = nltk.WordNetlemmatiser()</span><br><span class="line">ps = nltk.PorterStemmer()</span><br></pre></td></tr></table></figure>
<h3 id="comparation-between-lemmatiser-and-stemmer"><a class="markdownIt-Anchor" href="#comparation-between-lemmatiser-and-stemmer"></a> Comparation between Lemmatiser and Stemmer</h3>
<h4 id="first-example"><a class="markdownIt-Anchor" href="#first-example"></a> First Example</h4>
<p>Stemmer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(ps.stem(<span class="string">"meanness"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"meaning"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean</span><br><span class="line">mean</span><br></pre></td></tr></table></figure>
<p>As we have mentioned that, meanness and meaning are different but stemmer still cuts off and return root words for both words.</p>
<p>Lemmartiser:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(wn.lemmatise(<span class="string">"meanness"</span>))</span><br><span class="line">print(wn.lemmatise(<span class="string">"meaning"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">meanness</span><br><span class="line">meaning</span><br></pre></td></tr></table></figure>
<p>Two things need to be highlighted here.  First, stemming uses the algorithmic approach so it’s only concerned with the string that it’s given, and it will essentially chop off the suffix.  lemmatising is a little bit more complex in that it searches the corpus to find related words and condense it down to the core concept.  The problem is that if this word isn’t in the corpus, then it will just return the original word, so that’s what’s happening in this example.  With that said, not condensing it in this case is probably better than incorrectly stemming it using the Porter stemmer.</p>
<h4 id="second-example"><a class="markdownIt-Anchor" href="#second-example"></a> Second Example</h4>
<p>Stemmer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(ps.stem(<span class="string">"goose"</span>))</span><br><span class="line">print(ps.stem(<span class="string">"geese"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">goos</span><br><span class="line">gees</span><br></pre></td></tr></table></figure>
<p>You can see that the stemmer doesn’t quite know what to do here with goose and geese so it returns two different root words. So again, Python will still view goose and geese as two different things even if you use the stemmer.</p>
<p>Lemmartiser:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(wn.lemmatise(<span class="string">"goose"</span>))</span><br><span class="line">print(wn.lemmatise(<span class="string">"geese"</span>))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">goose</span><br><span class="line">goose</span><br></pre></td></tr></table></figure>
<p>You can see that the lemmatiser correctly maps both of these back to goose.  Python will then be able to now realize that these are the same words, so a lemmatiser can be quite powerful in some relatively complex situations.</p>
<h3 id="apply-lammatiser-to-sms-spam-collection-data"><a class="markdownIt-Anchor" href="#apply-lammatiser-to-sms-spam-collection-data"></a> Apply lammatiser to SMS Spam Collection Data</h3>
<p>Read the text file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">"SMSSpamCollection.tsv"</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">data.columns = [<span class="string">'label'</span>, <span class="string">'body_text'</span>]</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
</tr>
</tbody>
</table>
<p>Clean up text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = <span class="string">""</span>.join([word <span class="keyword">for</span> word <span class="keyword">in</span> text <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation])</span><br><span class="line">    tokens = re.split(<span class="string">'\W+'</span>, text)</span><br><span class="line">    text = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokens <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">'body_text_nostop'</span>] = data[<span class="string">'body_text'</span>].apply(<span class="keyword">lambda</span> x: clean_text(x.lower()))</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
<td style="text-align:center">[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr…</td>
</tr>
</tbody>
</table>
<p>Lemmatise text using body_text_nostop data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lemmatising</span><span class="params">(tokenised_text)</span>:</span></span><br><span class="line">    text = [wn.lemmatise(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokenised_text]</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">data[<span class="string">"body_text_lammatised"</span>] = data[<span class="string">"body_text_nostop"</span>].apply(<span class="keyword">lambda</span> x : lemmatising(x))</span><br><span class="line"></span><br><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">label</th>
<th style="text-align:center">body_text</th>
<th style="text-align:center">body_text_nostop</th>
<th style="text-align:center">body_text_lammatised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive …</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
<td style="text-align:center">[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv…</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Nah I dont think he goes to usf he lives around here though</td>
<td style="text-align:center">[nah, dont, think, goes, usf, lives, around, though]</td>
<td style="text-align:center">[nah, dont, think, go, usf, life, around, though]</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">Even my brother is not like to speak with me They treat me like aids patent.</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aids, patent]</td>
<td style="text-align:center">[even, brother, like, speak, treat, like, aid, patent]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I HAVE A DATE ON SUNDAY WITH WILL!!</td>
<td style="text-align:center">[date, sunday]</td>
<td style="text-align:center">[date, sunday]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call…</td>
<td style="text-align:center">[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr…</td>
<td style="text-align:center">[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre…</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c…</td>
<td style="text-align:center">[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170…</td>
<td style="text-align:center">[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170…</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came…</td>
<td style="text-align:center">[mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile…</td>
<td style="text-align:center">[mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, …</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">ham</td>
<td style="text-align:center">I’m gonna be home soon and i don’t want to talk about this stuff anymore tonight, k? I’ve cried …</td>
<td style="text-align:center">[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>
<td style="text-align:center">[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, …</td>
<td style="text-align:center">[six, chances, win, cash, 100, 20000, pounds, txt, csh11, send, 87575, cost, 150pday, 6days, 16,…</td>
<td style="text-align:center">[six, chance, win, cash, 100, 20000, pound, txt, csh11, send, 87575, cost, 150pday, 6days, 16, t…</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">spam</td>
<td style="text-align:center">URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM…</td>
<td style="text-align:center">[urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk…</td>
<td style="text-align:center">[urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk…</td>
</tr>
</tbody>
</table>
<p>Just like the stemmer, the lemmatiser won’t do particularly well with slang or abbreviations, so it’s not ideal for this data set.  It might be much more effective if it was used on a collection of book reports or journal articles.  There are a couple things that the lemmatiser was able to impact.  It transitioned this lives down into life, and it also transitioned mobiles into mobile, and so you’ll notice that these aren’t super interesting examples necessarily, but as you saw above, the lemmatiser can do some relatively sophisticated things, at least more sophisticated than the stemmer.  Now you’ve learned what lemmatising is, and how to actually apply it, so both stemming and lemmatising helps us reduce the corpus of words that the model is exposed to, and it explicitly correlates words with similar meaning.  The lemmatiser is typically more accurate than the stemmer but the trade-off is that it takes a little bit longer to run.  Based on your machine learning pipeline, if the lemmatiser is going to be a bottleneck, then you may opt for the more simple stemmer.</p>
<h1 id="vectorising-raw-data"><a class="markdownIt-Anchor" href="#vectorising-raw-data"></a> Vectorising Raw Data</h1>
<h2 id="introducing-vectorising"><a class="markdownIt-Anchor" href="#introducing-vectorising"></a> Introducing Vectorising</h2>
<p>The process that we use to convert text to a form that Python and a machine learning model can understand is called vectorizing.  This is defined as the process of encoding text as integers to create feature vectors.  Now if you don’t have much machine learning experience, you may be wondering what a feature vector is.  A feature vector is an n-dimensional vector of numerical features that represent some object.  So in our context, that means we’ll be taking an individual text message and converting it to a numeric vector that represents that text message.</p>
<p>Based on the following example, what we’re doing when we vectorize text is we’re taking this dataset that has one line per document with the cell entry as the actual text message and then we’re converting it to a matrix that still has one line per document, but then you have every word used across all documents as the columns of your matrix.  And then within each cell is counting how many times that certain word appeared in that document.  And this is called your document term matrix.  We’ll be referring to this term quite a bit.  Then once we have this numeric representation of each text message, then we can carry on down the pipeline and fit and train a model.</p>
<p>| body_text | call | claim | free | txt | label |<br>
| :-------: | :—: |<br>
| Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive … | 0 | 0 | 1 | 1 | spam |<br>
| Nah I dont think he goes to usf he lives around here though | 0 | 0 | 0 | 0 | ham |<br>
| Even my brother is not like to speak with me They treat me like aids patent. | 0 | 0 | 0 | 0 | ham |<br>
| I HAVE A DATE ON SUNDAY WITH WILL!! | 0 | 0 | 0 | 0 | ham |<br>
| As per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your call… | 0 | 0 | 0 | 0 | ham |<br>
| WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c… | 1 | 2 | 0 | 0 | spam |<br>
| Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came… | 1 | 0 | 2 | 0 | spam |<br>
| I’m gonna be home soon and i don’t want to talk about this stuff anymore tonight, k? I’ve cried … | 0 | 0 | 0 | 0 | ham |<br>
| SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, … | 0 | 0 | 0 | 1 | spam |<br>
| URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM… | 0 | 1 | 1 | 1 | spam |</p>
<p>To understand the motivation behind this process, as we mentioned previously that when looking at a word, Python only sees characters.  So we need to convert this into a format that Python can understand in order for our machine learning models to start to learn what certain words indicate about the overall sentence or document or label that we’re trying to predict.  So we vectorize this text to create a matrix that only has numeric entries.  So in our case, counting how many times each word appears in each text message.  The machine learning algorithm understands these counts.  So if it sees a one or a two or a three in a cell, then that model can start to correlate that with whatever we’re trying to predict.  In our case, that’s spam.  To roughly understand then, what the words, sentences, and documents represent.  So in our context, this means it can use how frequently these certain words appear to determine whether the individual text message is spam or not.</p>
<p>Original:</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">offer</th>
<th style="text-align:center">lol</th>
<th style="text-align:center">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
</tr>
</tbody>
</table>
<p>Filtered ham:</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">offer</th>
<th style="text-align:center">lol</th>
<th style="text-align:center">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">ham</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<p>Filtered Spam:</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">offer</th>
<th style="text-align:center">lol</th>
<th style="text-align:center">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">spam</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">spam</td>
</tr>
</tbody>
</table>
<p>So far, the post been talking about the entry of each cell in the document term matrix containing the count of how many times a given word appears in that text message, but that’s only one method of vectorization.  And that’s called, not surprisingly, count vectorization.  There are two other variations of count vectorization called N-grams and term frequency - inverse document frequency, which is often referred to as TF-IDF.  We’re going to cover each of these three methods of vectorization in more detail.</p>
<h2 id="count-vectorisation"><a class="markdownIt-Anchor" href="#count-vectorisation"></a> Count Vectorisation</h2>
<h2 id="n-gram-vectorising"><a class="markdownIt-Anchor" href="#n-gram-vectorising"></a> N-gram Vectorising</h2>
<h2 id="inverse-document-frequency-weighting"><a class="markdownIt-Anchor" href="#inverse-document-frequency-weighting"></a> Inverse Document Frequency Weighting</h2>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Zilan Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/">http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Data-Science/">Data Science    </a><a class="post-meta__tags" href="/tags/Data-Processing/">Data Processing    </a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning    </a><a class="post-meta__tags" href="/tags/AI/">AI    </a></div><div class="post_share"><div class="social-share" data-image="https://live.staticflickr.com/65535/48621182898_91a9c4cc04_o.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpeg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpeg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/08/20/Spark-for-Machine-Learning-AI/"><img class="next_cover lozad" data-src="https://live.staticflickr.com/65535/48621537921_7855b8a711_o.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Spark for Machine Learning and AI</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/08/20/Spark-for-Machine-Learning-AI/" title="Spark for Machine Learning and AI"><img class="relatedPosts_cover lozad" data-src="https://live.staticflickr.com/65535/48621537921_7855b8a711_o.jpg"><div class="relatedPosts_title">Spark for Machine Learning and AI</div></a></div><div class="relatedPosts_item"><a href="/2019/08/19/Hadoop-Environment-Setup/" title="Hadoop Environment Setup"><img class="relatedPosts_cover lozad" data-src="https://live.staticflickr.com/65535/48621680177_6107d2deaf_o.png"><div class="relatedPosts_title">Hadoop Environment Setup</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://hoanjinan.github.io/2019/09/02/Natural-Language-Processing/';
  this.page.identifier = '2019/09/02/Natural-Language-Processing/';
  this.page.title = 'Natural Language Processing (NLP)';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'hoanjinan-otoko' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Zilan Huang</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#introduction"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> Introduction</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introduction-to-the-post"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text"> Introduction to The Post</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introduction-to-natural-language-processing-nlp"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text"> Introduction to Natural Language Processing (NLP)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introduction-to-nltk"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text"> Introduction to NLTK</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#nlp-basics"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> NLP Basics</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#how-to-install-nltk-on-local-machine"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> How to install NLTK on local machine</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#download-nltk-data"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text"> Download NLTK data</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#reading-in-text-data"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text"> Reading in Text Data</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#what-is-unstructured-data"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text"> What is Unstructured Data</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#read-semi-structured-data"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text"> Read Semi-structured data</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#exploring-the-dataset"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text"> Exploring The Dataset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#regular-expressions"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text"> Regular Expressions</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#introduction-to-regular-expression"><span class="toc_mobile_items-number">2.4.1.</span> <span class="toc_mobile_items-text"> Introduction to Regular Expression</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#how-to-use-regular-expressions"><span class="toc_mobile_items-number">2.4.2.</span> <span class="toc_mobile_items-text"> How to Use Regular Expressions</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#regular-expression-replacements"><span class="toc_mobile_items-number">2.4.3.</span> <span class="toc_mobile_items-text"> Regular Expression Replacements</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#other-examples-of-regex-methods"><span class="toc_mobile_items-number">2.4.3.1.</span> <span class="toc_mobile_items-text"> Other examples of regex methods</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#machine-learning-pipeline"><span class="toc_mobile_items-number">2.5.</span> <span class="toc_mobile_items-text"> Machine Learning Pipeline</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#implementation"><span class="toc_mobile_items-number">2.6.</span> <span class="toc_mobile_items-text"> Implementation</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#removing-punctuation"><span class="toc_mobile_items-number">2.6.1.</span> <span class="toc_mobile_items-text"> Removing Punctuation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#tokenisation"><span class="toc_mobile_items-number">2.6.2.</span> <span class="toc_mobile_items-text"> Tokenisation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#removing-stop-words"><span class="toc_mobile_items-number">2.6.3.</span> <span class="toc_mobile_items-text"> Removing Stop Words</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#supplemental-data-cleaning"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> Supplemental Data Cleaning</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introducting-stemming"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text"> Introducting Stemming</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#using-stemming"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text"> Using Stemming</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introducting-lemmatising"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text"> Introducting Lemmatising</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#using-lemmatising"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text"> Using Lemmatising</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#test-out-wordnet-lemmatiser-read-more-about-wordnet-here"><span class="toc_mobile_items-number">3.4.1.</span> <span class="toc_mobile_items-text"> Test out WordNet lemmatiser (read more about WordNet here)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#comparation-between-lemmatiser-and-stemmer"><span class="toc_mobile_items-number">3.4.2.</span> <span class="toc_mobile_items-text"> Comparation between Lemmatiser and Stemmer</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#first-example"><span class="toc_mobile_items-number">3.4.2.1.</span> <span class="toc_mobile_items-text"> First Example</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#second-example"><span class="toc_mobile_items-number">3.4.2.2.</span> <span class="toc_mobile_items-text"> Second Example</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#apply-lammatiser-to-sms-spam-collection-data"><span class="toc_mobile_items-number">3.4.3.</span> <span class="toc_mobile_items-text"> Apply lammatiser to SMS Spam Collection Data</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#vectorising-raw-data"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> Vectorising Raw Data</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#introducing-vectorising"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text"> Introducing Vectorising</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#count-vectorisation"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text"> Count Vectorisation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#n-gram-vectorising"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text"> N-gram Vectorising</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#inverse-document-frequency-weighting"><span class="toc_mobile_items-number">4.4.</span> <span class="toc_mobile_items-text"> Inverse Document Frequency Weighting</span></a></li></ol></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" data-click="false"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":100,"vOffset":-10},"mobile":{"show":true},"log":false,"tagMode":false});</script></body></html>